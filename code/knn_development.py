# -*- coding: utf-8 -*-
"""KNN Development.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XWd4OPDX5lbbikS27Bb3LZQ66t9Mws3f

# Building ML Model
"""

import numpy as np
import sklearn
import pandas as pd
import tensorflow as tf
device_name = tf.test.gpu_device_name()
print(device_name)

import re
def Clean_Text(text): # preprocesses text
    text = str(text)
    text = re.sub(r'"','', text)
    text = re.sub(r'&amp','', text)
    text = re.sub(r'@[A-Za-z0-9_]+','@USER', text)#Remove's @'s
    text = re.sub(r'RT[\s]+','', text) # removes rt's
    text = re.sub(r'http?:\/\/\S+', 'HTTPURL', text)# removes hyperlinks
    text = re.sub(r'https?:\/\/\S+', 'HTTPURL', text)# removes hyperlinks
    text = re.sub(r'[&8216;]+','',text)#removes apostrophes that are miscoded
    text = re.sub(r'[&8217;]+','',text)#removes apostrophes that are miscoded
    text = re.sub(r'[\n]+','',text)#removes \n's
    text = re.sub(r'[0-9]+','',text)#removes all integers
    text = re.sub(r'[!]+','',text)
    text = re.sub(r':','',text)
    text = re.sub(r',',' ',text)
    text = re.sub(r'[.]+',' ',text)
    text = re.sub(r' +', ' ', text)
    text = " ".join(word.strip() for word in re.split('#|_', text))
    text = text.lower()
    lemmatizer = WordNetLemmatizer()
    text = lemmatizer.lemmatize(text)
    return text


def ChangeToBin_Davidson(integer):# changes davidson dataset to binary encoding as opposed to tri-class 
  if integer == 0:
    integer = 1
  else:
    integer = 0
  return integer

import tensorflow as tf
import nltk


from nltk.corpus import stopwords
nltk.download('stopwords')
stop = set(stopwords.words("english"))

def remove_stopwords(text):
  text = [word.lower() for word in text.split() if word.lower() not in stop]

  return " ".join(text)


df_train = pd.read_csv("train_tweet.csv")
df_test = pd.read_csv("labeled_data.csv")



df_train['text'] = df_train['tweet'].apply(Clean_Text)
df_train['text'] = df_train["text"].map(remove_stopwords)


df_train = df_train.drop(["id", "tweet"], axis = 1)



df_test['text'] = df_test['tweet'].apply(Clean_Text)
df_test['text'] = df_test["text"].map(remove_stopwords)
df_test["label"] = df_test["class"].apply(ChangeToBin_Davidson)



df_test = df_test.drop(['index', 'count', 'hate_speech', 'offensive_language', 'neither','class', 'tweet'], axis = 1)


frames = [df_train, df_test]
dataset = pd.concat(frames)

df_train.head()

hate_speech_test = df_test[df_test.label.eq(1)] # ensures balanced datasets
non_hate_speech_test = df_test[df_test.label.eq(0)]
hate_speech_train = df_train[df_train.label.eq(1)]
non_hate_speech_train = df_train[df_train.label.eq(0)]

equal_non_hate_speech_train = non_hate_speech_train.sample(len(hate_speech_train))

frames = [hate_speech_train,equal_non_hate_speech_train]

train_dataset = pd.concat(frames)

from sklearn import preprocessing
le = preprocessing.LabelEncoder() # encodes eachh tweet
with tf.device(device_name):
  

  new_encoded_dataset = le.fit_transform(train_dataset["text"])
  encoded_text_train = le.fit_transform(df_train["text"])
  encoded_text_test = le.fit_transform(df_test["text"])
  encoded_text_test_hate_speech = le.fit_transform(hate_speech_test["text"])
  encoded_text_test_non_hate_speech = le.fit_transform(non_hate_speech_test["text"])

from sklearn.neighbors import KNeighborsClassifier

# creates model and ensures it fits the data
model = KNeighborsClassifier(n_neighbors= 499, weights = 'uniform')
model.fit(new_encoded_dataset.reshape(-1,1), train_dataset["label"])

y_pred = model.predict(encoded_text_test_hate_speech.reshape(-1,1))

from sklearn.metrics import accuracy_score # computes accuracy versus hate speech
accuracy = accuracy_score(hate_speech_test["label"], y_pred)
print('Accuracy',accuracy)

y_pred = model.predict(encoded_text_test_non_hate_speech.reshape(-1,1))

from sklearn.metrics import accuracy_score # computes accuracy versus non hate speech set
accuracy = accuracy_score(non_hate_speech_test["label"], y_pred)
print('Accuracy',accuracy)

"""# Code for Testing Best K"""

import sys
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
import math
iterations = []
hate_accuracies = []
non_hate_accuracies = []

#tests best K from 1 to 500 using previous steps
with tf.device(device_name):
  for i in range(1,500):
    model = KNeighborsClassifier(n_neighbors=i, weights = "distance")
    model.fit(new_encoded_dataset.reshape(-1,1), train_dataset["label"])
    y_pred = model.predict(encoded_text_test_hate_speech.reshape(-1,1))
    hate_accuracy = accuracy_score(hate_speech_test["label"], y_pred)
    iterations.append(i)
    hate_accuracies.append(hate_accuracy)
    y_pred = model.predict(encoded_text_test_non_hate_speech.reshape(-1,1))
    non_hate_accuracy = accuracy_score(non_hate_speech_test["label"], y_pred)
    non_hate_accuracies.append(non_hate_accuracy)

from matplotlib import pyplot as plt
from matplotlib.pyplot import figure

# plots the hate speech and non-hate speech data

print("Average Hate Speech Accuracy", sum(hate_accuracies)/len(hate_accuracies))
print("Average Non Hate Speech Accuracy", sum(non_hate_accuracies)/len(non_hate_accuracies))

figure(figsize=(16, 10), dpi=100)
plt.plot(iterations, hate_accuracies, label = "hate_speech")
plt.plot(iterations, non_hate_accuracies, label = "non_hate_speech")
plt.legend()