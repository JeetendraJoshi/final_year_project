# -*- coding: utf-8 -*-
"""FinalSubmission_DLModels.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LFup1_GVAiVyfSTolufgPlkYgik1clQj
"""

!pip install datasets #installs required dependencies
!pip install --upgrade requests
!pip install https://github.com/amaiya/eli5/archive/refs/heads/tfkeras_0_10_1.zip

"""# Text Analysis

"""

import os
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from collections import Counter
from nltk.corpus import stopwords
from wordcloud import WordCloud, STOPWORDS


def wordBarGraphFunction(df,column,title): # allows every word in a specific column of a dataframe to be counted as long as it is not a stopword
    topic_words = [ z.lower() for y in
                       [ x.split() for x in df[column] if isinstance(x, str)]
                       for z in y]
    word_count_dict = dict(Counter(topic_words))
    popular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)
    popular_words_nonstop = [w for w in popular_words if w not in stopwords.words("english")]
    plt.barh(range(50), [word_count_dict[w] for w in reversed(popular_words_nonstop[0:50])])
    plt.yticks([x + 0.5 for x in range(50)], reversed(popular_words_nonstop[0:50]))
    plt.title(title)
    plt.show()

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
stop = set(stopwords.words("english"))

def ChangeToBin_Davidson(integer): # changes davidson dataset to binary encoding as opposed to tri-class
  if integer == 0:
    integer = 1
  else:
    integer = 0
  return integer

def remove_stopwords(text): # removes stopwords from a given input text
  text = [word.lower() for word in text.split() if word.lower() not in stop]

  return " ".join(text)

titles_table = pd.read_csv("labeled_data.csv")


titles_table["label"] = titles_table["class"].apply(ChangeToBin_Davidson)


hate_speech_test = titles_table[titles_table.label.eq(1)]



hate_speech_test = hate_speech_test.drop(['index', 'count', 'hate_speech', 'offensive_language', 'neither','class'], axis = 1)
plt.figure(figsize=(10,10))

wordBarGraphFunction(hate_speech_test,'tweet',"Most Popular Words in Hate Speech Dataset 1")

titles_table = pd.read_csv("train_tweet.csv")

titles_table = titles_table[titles_table.label.eq(1)]

plt.figure(figsize=(10,10))
wordBarGraphFunction(titles_table,'tweet',"Most Popular Words in Hate Speech Dataset 2")

"""# Building ML Model

"""

!pip install ktrain #installs ktrain package

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext autoreload
# %autoreload 2
# %matplotlib inline
import os #allows use of GPU for ktrain
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID";
os.environ["CUDA_VISIBLE_DEVICES"]="0";

#Initializes configuration variables

datasetType = 1
lemmatize = 1
class_weights = 1
remove_URL_User =1

import pandas as pd
import numpy as np
import ktrain
from ktrain import text
import re
import nltk
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer


def Clean_Text(text): # preprocesses the data
    text = str(text)
    text = re.sub(r'"','', text)
    text = re.sub(r'&amp','', text)
    text = re.sub(r'RT[\s]+','', text) # removes rt's


    if remove_URL_User == 1:
      text = re.sub(r'@[A-Za-z0-9_]+','', text)#Remove's @'s
      text = re.sub(r'http?:\/\/\S+', '', text)# removes hyperlinks
      text = re.sub(r'https?:\/\/\S+', '', text)# removes hyperlinks
    elif remove_URL_User == 2:
      text = re.sub(r'@[A-Za-z0-9_]+','@USER', text)#substitutes @'s
      text = re.sub(r'http?:\/\/\S+', 'HTTPURL', text)# substitutes hyperlinks
      text = re.sub(r'https?:\/\/\S+', 'HTTPURL', text)# substitutes hyperlinks


    text = re.sub(r'[&8216;]+','',text)#removes apostrophes that are miscoded
    text = re.sub(r'[&8217;]+','',text)#removes apostrophes that are miscoded
    text = re.sub(r'[\n]+','',text)#removes \n's
    text = re.sub(r'[0-9]+','',text)#removes all integers
    text = re.sub(r'[!]+','',text)# removes more than 1 exclamation point
    text = re.sub(r':','',text) # removes colons
    text = re.sub(r',',' ',text) # removes commas
    text = re.sub(r'[.]+',' ',text)# removes more than 1 full stop
    text = " ".join(word.strip() for word in re.split('#|_', text)) # converts hashtags into base words
    text = text.lower()
    if lemmatize == 2:
      lemmatizer = WordNetLemmatizer()
      text = lemmatizer.lemmatize(text)
    return text


def ChangeToBin_Davidson(integer): # changes davidson dataset to binary encoding as opposed to tri-class
  if integer == 0:
    integer = 1
  else:
    integer = 0
  return integer

import tensorflow as tf
import copy

from nltk.corpus import stopwords
nltk.download('stopwords')
stop = set(stopwords.words("english"))

def remove_stopwords(text): # removes stopwords
  text = [word.lower() for word in text.split() if word.lower() not in stop]

  return " ".join(text)


df_train = pd.read_csv("train_tweet.csv") # reads in the datasets
df_test = pd.read_csv("labeled_data.csv")



df_train['text'] = df_train['tweet'].apply(Clean_Text) # preprocesses the data


df_train = df_train.drop(["id", "tweet"], axis = 1) # removes unneccessary columns



df_test['text'] = df_test['tweet'].apply(Clean_Text)# preprocesses the data
df_test["label"] = df_test["class"].apply(ChangeToBin_Davidson)



df_test = df_test.drop(['index', 'count', 'hate_speech', 'offensive_language', 'neither','class', 'tweet'], axis = 1) # removes unneccessary columns

#negative -no hate speech. positive - hate speech
dataset = []
dataset = pd.concat([df_test, df_train], ignore_index = True) # fully combines bothh datasets


from sklearn.utils import class_weight # determines class weights fromm fully combined datasets
from collections import Counter
ctr = Counter(dataset['label'].values)
y = dataset['label'].values
cws = class_weight.compute_class_weight('balanced', classes = np.unique(y), y = y)
cws = dict(enumerate(cws))
train_set = []
test_set = []

if datasetType== 1: # datasets are created with a 90:10 split
  dataset['label'] = dataset['label'].apply(lambda x: 'neg' if x == 0 else 'pos')
  dataset = pd.concat([dataset, dataset.label.astype('str').str.get_dummies()], axis=1, sort=False)
  #negative -no hate speech. positive - hate speech
  dataset = dataset[['text', 'neg', 'pos']]
  train_set = dataset.sample(frac = 0.9, random_state = 200)
  test_set = dataset.drop(train_set.index)


if datasetType ==2 : # creates equivalently sized training dataset
  hate_speech_test = df_test[df_test.label.eq(1)]
  non_hate_speech_test = df_test[df_test.label.eq(0)]
  hate_speech_train = df_train[df_train.label.eq(1)]
  non_hate_speech_train = df_train[df_train.label.eq(0)]

  equal_non_hate_speech_train = non_hate_speech_train.sample(len(hate_speech_train))
  frames = [hate_speech_train,equal_non_hate_speech_train]


  train_set = pd.concat(frames)
  frames = [hate_speech_test, non_hate_speech_test]
  test_set = pd.concat(frames)

  train_set['label'] = train_set['label'].apply(lambda x: 'neg' if x == 0 else 'pos') # one hot encodes the data
  train_set = pd.concat([train_set, train_set.label.astype('str').str.get_dummies()], axis=1, sort=False)
  #negative -no hate speech. positive - hate speech
  train_set = train_set[['text', 'neg', 'pos']]
  train_set.head()

  test_set['label'] = test_set['label'].apply(lambda x: 'neg' if x == 0 else 'pos')# one hot encodes the data
  test_set = pd.concat([test_set, test_set.label.astype('str').str.get_dummies()], axis=1, sort=False)
  #negative -no hate speech. positive - hate speech
  test_set = test_set[['text', 'neg', 'pos']]
  test_set.head()




x_train = train_set["text"].values.tolist()
x_test = test_set["text"].values.tolist()
y_train = train_set[["neg", "pos"]].values.tolist()
y_test = test_set[["neg", "pos"]].values.tolist()

(x_train, y_train), (x_test, y_test), preproc = (0,0), (0,0), 0
# creates dataset objects required by ktrain
if datasetType != 1:
  (x_train, y_train), (x_test, y_test), preproc = text.texts_from_df(train_set,
                                                                    'text', # name of column containing review tweet
                                                                    label_columns=['neg', 'pos'],
                                                                    maxlen=280,
                                                                    val_df = test_set,
                                                                    max_features=100000,
                                                                    preprocess_mode='standard',
                                                                    val_pct=0.0,
                                                                    ngram_range=1)# only 1 for bigru model
else:
  (x_train, y_train), (x_test, y_test), preproc = text.texts_from_df(dataset,
                                                                   'text', # name of column containing review tweet
                                                                   label_columns=['neg', 'pos'],
                                                                   maxlen=280,
                                                                   max_features=100000,
                                                                   preprocess_mode='standard',
                                                                   val_pct=0.1,
                                                                   ngram_range=1)# only 1 for bigru model

model = text.text_classifier('bigru', (x_train, y_train) , preproc=preproc) # instantiates a model with the dataset
learner = ktrain.get_learner(model,
                             train_data=(x_train, y_train),
                             val_data=(x_test, y_test),
                             batch_size=32)

#### FOR BERT AND DISTILBERT

# import ktrain
# from ktrain import text
# MODEL_NAME = 'distilbert-base-uncased'
# t = text.Transformer(MODEL_NAME, maxlen=280, class_names=['neg', 'pos'])
# trn = t.preprocess_train(x_train, y_train)
# val = t.preprocess_test(x_test, y_test)
# model = t.get_classifier()
# learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=32)

#with tf.device(device_name):

if class_weights == 1:
  learner.lr_find(show_plot=True ) # used to find the optimal learning rate.
if class_weights == 2:
  learner.lr_find(show_plot=True, class_weight = cws )# used to find the optimal learning rate.

if class_weights == 1:
  learner.autofit(0.002,1) # used to train model with a triangular learning rate policy and no of epochs
if class_weights == 2:
  learner.autofit(0.002,1, class_weight = cws)# used to train model with a triangular learning rate policy and no of epochs

cm = learner.validate(print_report= True) # tests the model on a given validation set

learner.view_top_losses(n=5, preproc = preproc) # views top 5 losses of a model

p = ktrain.get_predictor(learner.model, preproc = preproc) #extracts the predictor with preprocessing and lets it be saved
p.save('/content/drive/MyDrive/FinalMLModels/gru')

p.explain("republicans aren't the only racist in america we have racist democrats too but don't play the token negro card httpurl") # allows contribution of a phrase to be explained

!zip -r /content/drive/MyDrive/FinalMLModels/gru.zip /content/drive/MyDrive/FinalMLModels/gru #zips the model

"""# Twitter Querying

"""

!pip install ktrain

import tweepy
import pandas as pd
from textblob import TextBlob
from wordcloud import WordCloud
import numpy as np
import re
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
import numpy as np
import ktrain
from ktrain import text
import re
import nltk
import tensorflow as tf
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
device_name = tf.test.gpu_device_name()
print(device_name)

ACCESS_TOKEN = '' #Access tokens for accessing Tweepy API(Redacted as tokens are only available to those with a Twitter Developer Account)
ACCESS_TOKEN_SECRET = ''
CONSUMER_KEY = ''
CONSUMER_SECRET = ''

#Create authentication
authenticate = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)
authenticate.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)

api = tweepy.API(authenticate, wait_on_rate_limit= True, wait_on_rate_limit_notify = True)

bert2211 = ktrain.load_predictor("/content/drive/MyDrive/FinalMLModels/bert2211")
bert2212 = ktrain.load_predictor("/content/drive/MyDrive/FinalMLModels/bert2212") # load models from relative path
distilbert = ktrain.load_predictor("/content/drive/MyDrive/FinalMLModels/distilbert")
gru = ktrain.load_predictor("/content/drive/MyDrive/FinalMLModels/gru")
nbsvm = ktrain.load_predictor("/content/drive/MyDrive/FinalMLModels/nbsvm")

def majority_rule(text): # defines majority rule function for machine learning models
  predictions = [bert2211.predict(text), bert2212.predict(text), distilbert.predict(text), gru.predict(text), nbsvm.predict(text)]
  positives = predictions.count("pos")
  if positives >= 3:
    return 1
  else:
    return 0

def Clean_Text(text): # Clean_Text used specifically for ensemble models
    text = str(text)
    text = re.sub(r'"','', text)
    text = re.sub(r'&amp','', text)
    text = re.sub(r'@[A-Za-z0-9_]+','@USER', text)#Remove's @'s
    text = re.sub(r'RT[\s]+','', text) # removes rt's
    text = re.sub(r'http?:\/\/\S+', 'HTTPURL', text)# removes hyperlinks
    text = re.sub(r'https?:\/\/\S+', 'HTTPURL', text)# removes hyperlinks
    text = re.sub(r'[&8216;]+','',text)#removes apostrophes that are miscoded
    text = re.sub(r'[&8217;]+','',text)#removes apostrophes that are miscoded
    text = re.sub(r'[\n]+','',text)#removes \n's
    text = re.sub(r'[0-9]+','',text)#removes all integers
    text = re.sub(r'[!]+','',text)
    text = re.sub(r':','',text)
    text = re.sub(r',',' ',text)
    text = re.sub(r'[.]+',' ',text)
    text = re.sub(r' +', ' ', text)
    text = " ".join(word.strip() for word in re.split('#|_', text))
    text = text.lower()
    lemmatizer = WordNetLemmatizer()
    text = lemmatizer.lemmatize(text)
    return text

def tenTweets_to_Hate_User(tweets): # converts ten input tweets to confirmation of a hate user based on threshold seven

  y_pred = 0
  for j in tweets:
    j = Clean_Text(j)
    y_pred+= majority_rule(j)
  if y_pred>= 7:
    return 1
  else:
    return 0

import random

word_list = ["bitch", "faggot", "ass", "white", "fuck", "nigga", "fucking", "nigger", "trash", "niggas", "hate", "bitch", "bitches"] # define word list

user_names = []
while len(user_names)== 0 or len(word_list) == 0 : # query random word from word list till users are found
  query_word = random.choice(word_list)
  word_list.remove(query_word)
  query = api.search(query_word, lang = "en")

  columns = ['User', 'Tweet']
  data = []

  for tweet in query:
    data.append([tweet.user.screen_name, tweet.text])

  df_query = pd.DataFrame(data , columns = columns)
  df_query['text'] = df_query['Tweet'].apply(Clean_Text) # clean dataframe
  y_pred = []

  for i, val in enumerate(df_query['text']):
    y_pred.append(majority_rule(val))# classifies tweet

  user_names = []

  for i, val in enumerate(y_pred):
    if val == 1:
      user_names.append(df_query["User"][i])

print(user_names)

hate_users = [] #classifies last ten tweets
for i in user_names:
    lastTenTweets = api.user_timeline(user_id = i, count = 10);
    tentweetstext = []
    for tweet in lastTenTweets:
      tentweetstext.append(tweet.text)
    result = tenTweets_to_Hate_User(tentweetstext)
    if result == 1:
      hate_users.append(i)

print(hate_users)

user_count = 10 # define how many friends and followers can be queried

def get_hate_user_stats_screen_name(hate_users):
  hate_user_stats = []
  for user in hate_users:
    try:
      hate_user_followers = api.followers_ids(screen_name = user, count = user_count)
      hate_user_friends = api.friends_ids(screen_name = user, count = user_count)
      hate_user_stats.append([user, hate_user_followers, hate_user_friends])
    except tweepy.TweepError:
      continue
  return hate_user_stats
def get_hate_user_stats_user_id(hate_users):
  hate_user_stats = []
  for user in hate_users:
    try:
      hate_user_followers = api.followers_ids(user_id = user, count = user_count)
      hate_user_friends = api.friends_ids(user_id = user, count = user_count)
      hate_user_stats.append([user, hate_user_followers, hate_user_friends])
    except tweepy.TweepError:
      continue
  return hate_user_stats

import networkx as nx
import matplotlib.pyplot as plt
depth = 3 # defines depth of graph(how deep the network can go in terms of friends and followers)
G = nx.MultiDiGraph()
red_edges = []
red_nodes = []
hate_user_stats = get_hate_user_stats_screen_name(hate_users) # gets hate user's friends and followers
with tf.device(device_name):

  for z in range(1 , depth):
    new_hate_users = []
    for i in hate_user_stats:
      prim_user = i[0]
      red_nodes.append(prim_user)
      hate_user_friends = i[1]
      hate_user_followers = i[2]
      print(prim_user)
      for j in hate_user_friends:
        G.add_edge(prim_user, j)
        try:
          lastTenTweets = api.user_timeline(user_id = j, count = 10);
          y_pred = 0
          for k in lastTenTweets:
            k = Clean_Text(k.text)
            y_pred += majority_rule(k)
          if y_pred >= 7:
            new_hate_users.append(j)
            red_edges.append((prim_user, j))
            red_nodes.append(j)
        except tweepy.TweepError:
          continue

      for j in hate_user_followers:
        G.add_edge(j, prim_user)
        try:
          lastTenTweets = api.user_timeline(user_id = j, count = 10);
          y_pred = 0
          for k in lastTenTweets:
            k = Clean_Text(k.text)
            y_pred+= majority_rule(k)
          if y_pred >= 7:
            new_hate_users.append(j)
            red_edges.append((j, prim_user))
            red_nodes.append(j)
        except tweepy.TweepError:
          continue

    new_hate_users = list(set(new_hate_users))
    print(new_hate_users)
    hate_user_stats = get_hate_user_stats_user_id(new_hate_users)

# draws network with correct colouring
edge_colours = ['black' if not edge in red_edges else 'red'
                for edge in G.edges()]
black_edges = [edge for edge in G.edges() if edge not in red_edges]

def_nodes =[node for node in G.nodes() if node not in red_nodes]

red_nodes = list(set(red_nodes))
fig, ax = plt.subplots(1 , 1 , figsize = (20,20), dpi = 120)
ax.grid(False)
pos = nx.spiral_layout(G)

nx.draw_networkx_nodes(G, pos, nodelist = def_nodes, cmap=plt.get_cmap('jet'), node_size = 100, ax = ax)
nx.draw_networkx_nodes(G, pos, nodelist = red_nodes, cmap=plt.get_cmap('jet'), node_size = 100, ax = ax, node_color = 'r')


nx.draw_networkx_edges(G, pos, edgelist=black_edges, arrows=True, ax = ax, arrowsize = 7)
nx.draw_networkx_edges(G, pos, edgelist=red_edges, edge_color='r', arrows=True, ax = ax, arrowsize = 10, width = 3)

print("Network:")
# fig.set_size_inches(100, 100)
#plt.figure(3,figsize=(15,15))
#nx.draw_networkx(G,pos, ax = ax)
# plt.draw()
# plt.show()

"""#Testing Area

"""

def changetoBinhateCheck(text):
  if text == "hateful":
    return 1
  else:
    return 0

test_df = pd.read_csv("test_suite_cases.csv")


test_df["label_gold"] = test_df["label_gold"].apply(changetoBinhateCheck)

test_df

tp = 0
tn = 0
fp = 0
fn = 0



for i, val in enumerate(test_df["test_case"]):
  result = majority_rule(val)
  print(i)
  if result == 1:
    if test_df["label_gold"][i] == 1:
      tp +=1
    else:
      fp +=1
  else:
    if test_df["label_gold"][i] == 0:
      tn +=1
    else:
      fn+=1

print("True Positives:", tp)
print("True Negatives:", tn)
print("False Positives:", fp)
print("False Negatives:", fn)

print("Hate Speech Accuracy: ", tp/(tp+fn))
print("Non-Hate Speech Accuracy: ", tn/(tn+fp))
print("Total Accuracy: ", (tp+tn)/(tp+tn+fn+fp))
